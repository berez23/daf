# Copyright 2017 TEAM PER LA TRASFORMAZIONE DIGITALE
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

akka {
  actor {
    test-pool-dispatcher {
      type = "Dispatcher"
      executor = "thread-pool-executor"

      thread-pool-executor {
        fixed-pool-size = 8
      }

      throughput = 4
    }

    test-dispatcher {
      type = "Dispatcher"

      throughput = 4
    }
  }
}

play {
  ws.ssl.loose {
    acceptAnyCertificate = "true"
    allowWeakCiphers     = "true"
  }

  modules.enabled += "daf.util.TestPac4JStoreModule"
}

kerberos {
  principal = "daf@DAF.GOV.IT"
  keytab    = "conf/daf.keytab"
}

kafka {
  servers = [
    "localhost:9092" # brokers' host:port
  ]
  group_id      = "group-1" # group id to use when consuming
  num_producers = 1 # sets the number of producer actors to have ready in the pool

  topic { # configuration settings used when creating a topic
    partitions         = 1 # number of partitions
    replication_factor = 1 # amount of replication to use per partition per topic
    cleanup_policy     = delete # indicates how to dispose of rolled log segments whose size or time retention limits have been exceeded [delete, compact]
    compression_type   = uncompressed # indicates what type of compression to use [uncompressed, snappy, lz4, gzip, producer]
    max_message_bytes  = 1048576 # maximum amount of bytes er message, default 1MB
    retention_time     = 7 days # maximum retention for a rolled log before triggering cleanup
  }
}

daf {

  catalog_url = "http://catalog-manager:9000"

  stream {
    validator = "avro-json" # sets the validator to use when reading transmitted events before pushing onto kafka [avro, avro-json, none]
  }
}